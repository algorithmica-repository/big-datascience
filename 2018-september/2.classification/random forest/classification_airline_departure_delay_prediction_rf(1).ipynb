{"cells":[{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoderEstimator\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom collections import defaultdict\nfrom pyspark.ml.feature import Imputer\nfrom pyspark.sql.functions import col,sum\nimport datetime\nfrom pyspark.ml import PipelineModel"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["def get_types(df):\n    print(df.schema)\n    print(df.schema.fields)\n    data_types_map = defaultdict(list)\n    for entry in df.schema.fields:\n      data_types_map[str(entry.dataType)].append(entry.name)\n    print(data_types_map)\n    return data_types_map\n\ndef filter_rows_with_missing_labels(df, target):\n    #print(df.count())\n    tmp = df.filter(df[target].isNotNull())\n    print(tmp.count())\n    return tmp\n  \ndef drop_features(df, features):\n    print(len(df.columns))\n    for feature in features:\n      df = df.drop(feature)\n    print(len(df.columns))\n    return df\n\ndef filter_features_with_missing_data(df):\n    tmp = df.filter(df[target].isNotNull())\n    return tmp\n  \ndef get_missing_info(df):\n    tmp = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n    display(tmp)\n\ndef eda_categorical_feature(df, feature):\n    tmp = df.groupby(feature).count().toPandas()\n    print(tmp)\n        \ndef eda_continuous_features(df, feature):\n    df.describe(feature).show()\n  \ndef cast_integer_features_as_double(df, data_types_map):\n    for c in data_types_map[\"IntegerType\"]:\n       df = df.withColumn(c, df[c].cast('double'))\n    df.printSchema()\n    return df\n        \ndef imputer_continuous_features(df, data_types_map):\n    continuous_features = list(set(data_types_map['DoubleType']) - set(['DEP_DEL15']))\n    continuous_features_imputed = [var + \"_imputed\" for var in  continuous_features]\n    imputer = Imputer(inputCols = continuous_features, outputCols = continuous_features_imputed)\n    tmp = imputer.fit(df).transform(df)\n    get_missing_info(tmp)\n    return [imputer]\n\ndef impute_categorical_features(df, data_types_map):\n    missing_data_fill = {}\n    for var in data_types_map['StringType']:\n      missing_data_fill[var] = \"missing\"\n    tmp = df.fillna(missing_data_fill)\n    get_missing_info(tmp)\n    return tmp\n    \ndef encoder_categorical_features(df, data_types_map):\n    categorical_features = list(set(data_types_map['StringType']) - set(['_c46']))\n    string_indexers = [StringIndexer(inputCol=feature, outputCol=feature+\"_index\") for feature in categorical_features]\n\n    ohe_input_features = [ feature+\"_index\" for feature in categorical_features]\n    ohe_output_features = [ feature+\"_encoded\" for feature in categorical_features]\n    ohe_encoder = OneHotEncoderEstimator(inputCols=ohe_input_features, outputCols=ohe_output_features)  \n    pipeline = Pipeline(stages=string_indexers + [ohe_encoder])\n    tmp = pipeline.fit(df).transform(df)\n    tmp.printSchema()\n    return string_indexers + [ohe_encoder]\n\ndef assembler_for_feature_vector(df, data_types_map):\n    features = []\n    #categorical_features = list(set(data_types_map['StringType']) - set(['_c46']))\n    #features = features + [ feature+\"_encoded\" for feature in categorical_features]\n    \n    continuous_features = list(set(data_types_map['DoubleType']) - set(['DEP_DEL15']))\n    features = features + [var + \"_imputed\" for var in  continuous_features]\n    print(features)\n    assembler = VectorAssembler(inputCols= features, outputCol=\"features\")\n    #tmp = assembler.transform(df)\n    #tmp.printSchema()\n    return [assembler]\n\ndef split_data(df, train_percent):\n    return df.randomSplit([train_percent, 1-train_percent], seed=123);\n\ndef build_and_tune_model_with_cv(estimator, param_grid, evaluatior, df):\n    # define grid based cross validator\n    crossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=param_grid,\n                          evaluator= evaluator,\n                          numFolds=2)\n\n    # train model using fit\n    cv_model = crossval.fit(df)\n    return cv_model\n  \ndef model_summary_rf(cv_model):\n    print(cv_model)\n    print(cv_model.avgMetrics)\n    best_model = cv_model.bestModel\n    print(best_model)\n    print(best_model.stages)\n    rf_model = best_model.stages[-1]\n    print(rf_model)\n    print(rf_model.featureImportances)\n    print(rf_model.trees)\n    print(rf_model.treeWeights)\n    for tree in rf_model.trees:\n      print(tree.toDebugString)\n  \ndef predict(cv_model, evaluator, df):\n    # evaluate model on test set\n    predictions = cv_model.transform(df)\n    print(predictions)\n    predictions.printSchema()\n    print(evaluator.evaluate(predictions))\n\ndef persist_model(cv_model, type, model_dir):\n    datestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\n    file_name = \"cv_model_\" + type + \"_\" + datestamp\n    path = model_dir + file_name\n    print(path)\n    cv_model.bestModel.save(path)\n    \ndef load_model(path):\n    return PipelineModel.load(path)\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["airline_delays = spark.read.load(\"/FileStore/tables/airline_delays.csv\",\n                             format=\"csv\", header=\"true\", inferSchema=\"true\", sep=\",\")\nairline_delays.printSchema()\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(airline_delays)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["print(airline_delays.count())\ndata_types_map = get_types(airline_delays)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["airline_delays = cast_integer_features_as_double(airline_delays, data_types_map)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["airline_delays.printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["target = 'DEP_DEL15'\neda_categorical_feature(airline_delays, target)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["airline_delays_not_null = filter_rows_with_missing_labels(airline_delays, target)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["eda_categorical_feature(airline_delays_not_null, target)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#split the data into train and validation sets\nairline_delays_train, airline_delays_test = split_data(airline_delays_not_null, 0.75)\n\n#persist the train and test frames\nairline_delays_train.persist(); \nprint(airline_delays_train.count());\nairline_delays_test.persist(); \nprint(airline_delays_test.count());\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["get_missing_info(airline_delays_train)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#impute categorical features\nairline_delays_train = impute_categorical_features(airline_delays_train, data_types_map)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#imputer for continuous features\nimputer = imputer_continuous_features(airline_delays_train, data_types_map)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#encoder for categorical features\nencoder = encoder_categorical_features(airline_delays_train, data_types_map)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#assembler for vector data\nassembler = assembler_for_feature_vector(airline_delays_train, data_types_map)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["pipeline = Pipeline(stages=imputer + encoder + assembler)\ntmp = pipeline.fit(airline_delays_train).transform(airline_delays_train)\ntmp.printSchema()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#define the estimator\nrandForest = RandomForestClassifier(featuresCol='features', labelCol = target)\n\n# define the modeling pipeline with formula + feature transofrmations + estimator\npipeline = Pipeline(stages=imputer + encoder + assembler + [randForest])\n\n#define binary classification evaluator with right metric\nevaluator = BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderROC\")\n\n# Define the parameter grid for random forest\nparam_grid = ParamGridBuilder() \\\n    .addGrid(randForest.numTrees, [10]) \\\n    .addGrid(randForest.maxDepth, [3]) \\\n    .build()\n\ncv_model = build_and_tune_model_with_cv(pipeline, param_grid, evaluator, airline_delays_train)\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["model_summary_rf(cv_model)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["predict(cv_model, evaluator, airline_delays_test)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["persist_model(cv_model, \"rf\", \"/FileStore/tables/\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["loaded_model = load_model(\"/FileStore/tables/CV_Model_rf_10-06-2018-1538862687\")\nprint(loaded_model)\npredict(loaded_model, evaluator, airline_delays_test)"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"classification_airline_departure_delay_prediction_rf","notebookId":2677066382502744},"nbformat":4,"nbformat_minor":0}
